#!/usr/bin/env python3
"""
Scrapy Manager - Controle avanÃ§ado do scraping B2Drop
Integra Scrapy com nossa arquitetura existente
"""

import os
import sys
import subprocess
import argparse
import time
from datetime import datetime
from pathlib import Path

# Adiciona paths necessÃ¡rios
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from loguru import logger
from cloudflare_storage_fixed import CloudflareStorage


class ScrapyManager:
    """Gerenciador principal do Scrapy para B2Drop"""
    
    def __init__(self):
        self.project_dir = project_root / "b2drop_scraper"
        self.logs_dir = project_root / "logs"
        self.storage = None
        
        # Cria diretÃ³rios necessÃ¡rios
        self.logs_dir.mkdir(exist_ok=True)
        (self.project_dir / "logs").mkdir(exist_ok=True)
        
        logger.info("ğŸš€ Scrapy Manager inicializado")
    
    def check_environment(self):
        """Verifica se ambiente estÃ¡ configurado corretamente"""
        logger.info("ğŸ” Verificando ambiente...")
        
        # Verifica variÃ¡veis de ambiente
        required_vars = [
            'CLOUDFLARE_API_TOKEN',
            'CLOUDFLARE_ACCOUNT_ID', 
            'CLOUDFLARE_KV_NAMESPACE_ID',
            'CLOUDFLARE_R2_BUCKET_NAME',
            'CLOUDFLARE_R2_PUBLIC_DOMAIN'
        ]
        
        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars:
            logger.error(f"âŒ VariÃ¡veis de ambiente faltando: {missing_vars}")
            return False
        
        # Testa conexÃ£o Cloudflare
        try:
            self.storage = CloudflareStorage()
            if not self.storage.init_r2_bucket():
                logger.error("âŒ Falha ao conectar com Cloudflare")
                return False
        except Exception as e:
            logger.error(f"âŒ Erro na conexÃ£o Cloudflare: {e}")
            return False
        
        # Verifica se projeto Scrapy existe
        if not (self.project_dir / "scrapy.cfg").exists():
            logger.error(f"âŒ Projeto Scrapy nÃ£o encontrado em {self.project_dir}")
            return False
        
        logger.info("âœ… Ambiente verificado com sucesso")
        return True
    
    def run_spider(self, spider_name="b2drop", **kwargs):
        """Executa spider Scrapy"""
        if not self.check_environment():
            return False
        
        logger.info(f"ğŸ•·ï¸ Iniciando spider: {spider_name}")
        
        # Prepara comando Scrapy
        cmd = [
            "scrapy", "crawl", spider_name,
            "-L", "INFO",
            f"-s", f"LOG_FILE={self.logs_dir}/scrapy_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        ]
        
        # Adiciona parÃ¢metros extras
        for key, value in kwargs.items():
            cmd.extend(["-a", f"{key}={value}"])
        
        # Executa comando
        try:
            logger.info(f"ğŸ“ Executando: {' '.join(cmd)}")
            
            process = subprocess.run(
                cmd,
                cwd=self.project_dir,
                capture_output=False,
                text=True,
                timeout=3600  # Timeout de 1 hora
            )
            
            if process.returncode == 0:
                logger.info("âœ… Spider executado com sucesso")
                return True
            else:
                logger.error(f"âŒ Spider falhou com cÃ³digo: {process.returncode}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error("âŒ Spider excedeu tempo limite (1 hora)")
            return False
        except Exception as e:
            logger.error(f"âŒ Erro ao executar spider: {e}")
            return False
    
    def get_statistics(self):
        """ObtÃ©m estatÃ­sticas dos produtos armazenados"""
        if not self.storage:
            try:
                self.storage = CloudflareStorage()
                self.storage.init_r2_bucket()
            except:
                logger.error("âŒ NÃ£o foi possÃ­vel conectar ao Cloudflare")
                return None
        
        logger.info("ğŸ“Š Coletando estatÃ­sticas...")
        stats = self.storage.get_statistics()
        
        if stats and stats.get('total', 0) > 0:
            logger.info("="*50)
            logger.info("ğŸ“ˆ ESTATÃSTICAS ATUAIS")
            logger.info("="*50)
            logger.info(f"ğŸ“¦ Total de produtos: {stats['total']}")
            logger.info(f"ğŸ’° PreÃ§o mÃ©dio: R$ {stats.get('preco_medio', 0):.2f}")
            logger.info(f"ğŸ’° PreÃ§o mÃ­nimo: R$ {stats.get('preco_min', 0):.2f}")
            logger.info(f"ğŸ’° PreÃ§o mÃ¡ximo: R$ {stats.get('preco_max', 0):.2f}")
            
            if stats.get('categorias'):
                logger.info("ğŸ·ï¸ Produtos por categoria:")
                for cat, count in stats['categorias'].items():
                    logger.info(f"   â€¢ {cat}: {count}")
            
            logger.info("="*50)
        else:
            logger.warning("âš ï¸ Nenhuma estatÃ­stica disponÃ­vel")
        
        return stats
    
    def compare_with_existing(self):
        """Compara dados atuais com sistema legado"""
        logger.info("ğŸ” Comparando com dados existentes...")
        
        # Executa listagem do sistema atual
        try:
            result = subprocess.run([
                "python", "import_to_cloudflare.py", "stats"
            ], capture_output=True, text=True, cwd=project_root)
            
            if result.returncode == 0:
                logger.info("ğŸ“Š Dados do sistema atual:")
                for line in result.stdout.split('\n'):
                    if 'Total de produtos:' in line or 'PreÃ§o' in line:
                        logger.info(f"   {line.strip()}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ NÃ£o foi possÃ­vel acessar sistema atual: {e}")
    
    def scheduled_run(self, interval_minutes=60):
        """Executa spider em intervalos regulares"""
        logger.info(f"â° Iniciando execuÃ§Ã£o agendada (a cada {interval_minutes} minutos)")
        
        while True:
            try:
                logger.info("ğŸ”„ Iniciando execuÃ§Ã£o agendada...")
                
                success = self.run_spider()
                if success:
                    self.get_statistics()
                    self.compare_with_existing()
                
                logger.info(f"ğŸ˜´ Aguardando {interval_minutes} minutos...")
                time.sleep(interval_minutes * 60)
                
            except KeyboardInterrupt:
                logger.info("â¹ï¸ ExecuÃ§Ã£o agendada interrompida pelo usuÃ¡rio")
                break
            except Exception as e:
                logger.error(f"âŒ Erro na execuÃ§Ã£o agendada: {e}")
                logger.info(f"ğŸ”„ Tentando novamente em {interval_minutes} minutos...")
                time.sleep(interval_minutes * 60)


def main():
    """FunÃ§Ã£o principal CLI"""
    parser = argparse.ArgumentParser(description="Scrapy Manager para B2Drop")
    
    subparsers = parser.add_subparsers(dest='command', help='Comandos disponÃ­veis')
    
    # Comando: run
    run_parser = subparsers.add_parser('run', help='Executa spider uma vez')
    run_parser.add_argument('--spider', default='b2drop', help='Nome do spider')
    
    # Comando: schedule
    schedule_parser = subparsers.add_parser('schedule', help='Executa spider em intervalos')
    schedule_parser.add_argument('--interval', type=int, default=60, help='Intervalo em minutos')
    
    # Comando: stats
    stats_parser = subparsers.add_parser('stats', help='Mostra estatÃ­sticas')
    
    # Comando: check
    check_parser = subparsers.add_parser('check', help='Verifica ambiente')
    
    # Comando: compare
    compare_parser = subparsers.add_parser('compare', help='Compara com dados existentes')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = ScrapyManager()
    
    if args.command == 'run':
        manager.run_spider(args.spider)
    elif args.command == 'schedule':
        manager.scheduled_run(args.interval)
    elif args.command == 'stats':
        manager.get_statistics()
    elif args.command == 'check':
        success = manager.check_environment()
        sys.exit(0 if success else 1)
    elif args.command == 'compare':
        manager.compare_with_existing()


if __name__ == "__main__":
    main()